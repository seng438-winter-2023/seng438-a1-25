>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 25      |
|-----------------|
| Sahil Bhatt                |   
| Harshal Patel              |   
| Siwon Kim               |   
| Abhiraam Manchiraju                |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction](#introduction)

[2 High-level description of the exploratory testing plan](#high-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing](#comparison-of-exploratory-and-manual-functional-testing)

[4 Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)

[5 How the pair testing was managed and team work/effort was
divided](#how-the-pair-testing-was-managed-and-team-workeffort-was-divided)

[6 Difficulties encountered, challenges overcome, and lessons
learned](#difficulties-encountered-challenges-overcome-and-lessons-learned)

[7 Comments/feedback on the lab and lab document itself](#commentsfeedback-on-the-lab-and-lab-document-itself)

# Introduction

The purpose of this lab is to gain a comprehensive understanding of software testing. Within this, we applied a variety of testing techniques such as exploratory testing, manual scripted testing and regression testing. We were also familiarized with industrial defect tracking systems, processes and practices. 

Going into this lab, we had some prior knowledge pertaining to the practices of exploratory and manual functional testing. With exploratory testing, tests are created and executed simultaneously while following no particular structured approach. This gives the tester freedom to play around with the software on hand and create realistic scenarios an actual user might come across. With manual functional testing, the basic principle involves following a structured approach where the tester follows a laid out test suite and simply verifies if each test passes or fails.


# High-level description of the exploratory testing plan

Our approach for testing the functionality of the system involves testing each aspect individually for a brief period of time. Since there are roughly five main functions (logging in and four transaction types), we will spend an even split of approximately six minutes per function and attempt to test as many different input combinations as possible. By splitting into each of these sections, we will be able to test a variety of categories such as ensuring GUI elements are updated and function logic is performed correctly. Each of these sections will also be tested in two phases. The first phase will involve working with a series of normal inputs to simulate an ordinary user using the system. This will be followed by the next phase where we will work with more unconventional inputs to simulate users who may make mistakes in inputs or attempt to break the system.

# Comparison of exploratory and manual functional testing

### **Exploratory Testing** 
Exploratory testing is a human-based form of testing where tests are created and executed simutaneously. This form of testing relies on creative freedom of the tester and their own ability to create constructive tests. The tester will attempt to investigate the program, and explore the limits of the system’s functionalities. Through this process, the tester will be able to further grasp the program’s functionalities and be able to constitute new and more refined tests. The primary benefit of exploratory testing is the ability to uncover bugs that conventional scripted testing usually misses. Since exploratory tests are not constrained to the same conventions of scripted testing, the tester can often test a greater variation of scenarios and cases. This allows exploratory testing to excel at finding edge cases that lead to quality failure. This was evident when each pair performed the exploratory testing phase. We were able to find many edge cases, especially in regards to testing invalid inputs, that the manual functioning tests did not cover. Furthermore, exploratory testing can often be completed at a much faster pace than conventional scripted testing. Scripted testing often requires greater setup including carefully creating both the test cases and test themselves. Since we did not need to create these scripted tests ourselves, overall both approach required similar time, but we can infer that if these scripted tests were to be created ourselves, the time needed to complete these tests would be exponentially longer in comparison to an exploratory approach. As new software engineers with very little testing experience, we noticed there were many tests missing within our exploratory approach in comparison to the scripted tests. Furthermore, since exploratory testing is often based on the tester’s spontaneous decisions and actions, recreations of the test itself is often difficult. Thus, during our exploratory approach when discovering a bug, we were not able to accurately rediscover the exact inputs and decisions to replicate the bug.

### **Manual Functional Testing**
Manual functional testing is a series of predetermined test cases meant to test various use-cases of the application. These are tests that were pre-planned by the development team for how they think a user may use the product. Utilizing this process, the developers can ensure that the application is working for its main functionalities, as well as the reasons for which it was designed. Scripted tests account for the majority of the cases that the application will be used for, but may not account for all possible use cases of the program, especially some edge cases. Scripted testing also does not require much knowledge of the product to execute, as all that the tester needs to do is follow the test instructions and compare the actual and expected outputs. This is in contrast to exploratory testing, where the tester needs to have a vast knowledge of the product and some creativity to test the edge cases of the product. Some of the disadvantages of scripted testing is that it may not account for all the use cases of the program. Additionally, it requires a lot more setup and thought to design the tests and the test cases, rather than just randomly using the program and looking for bugs. It also is limited because it is subjective to the developer’s view of how the program should be used, and does not really account for user creativity of using the program for something else.


# Notes and discussion of the peer reviews of defect reports
Once the pair testing finished both groups got together to compare the defect reports, we carefully went through each of the bugs that both pairs encountered. We went through the bugs carefully to see which bugs both pairs found, along with bugs a particular pair found. Both of the pairs had different approaches to finding bugs, and because of this we found it important to carefully discuss the bugs we found. We also discussed areas we might have neglected or did not cover extensively as we don’t want to miss any bugs. Notably, we found that both groups did not  test Money Market as extensively as we could have with both accounts, and we found bugs with that feature. With this plan, we made sure that all group members had a sound understanding of the bugs in the system, and how the bugs could be recreated. 

# How the pair testing was managed and team work/effort was divided 
The lab work was divided evenly amongst all group members. The two teams, Abhiraam and Siwon and, Sahil and Harshal worked independently in their groups during the exploratory testing period. Both teams followed a similar approach during this phase. One member was responsible for testing the application and attempting to discover bugs while the other member was responsible to keep track of the steps taken and write a detailed description for the discovered bug. During manual functional testing, all group members got together to work on the section. We had two members run the program, working on the same test at the same time. This was to ensure that when a bug was discovered, it wasn't due to the individual's mistakes during testing. The other group members would play a management role in the tests, such as keeping track of which tests have been performed, reporting any defects found and determining what order to execute tests in. As for regression testing, we followed a similar approach to our manual functional testing. 

# Difficulties encountered, challenges overcome, and lessons learned
### **Difficulties**
Some of the difficulties we faced in this lab were navigating the application, contextualizing bugs, and setting up Jira. Since the instructions on the assignment were moderately vague, we initially had trouble understanding the application and navigating through the program. Furthermore, as we  proceeded doing exploratory testing, we often had trouble classifying what the bugs were, because they were often encapsulated in each other. Another issue we had was attempting to create a new bug report, Jira’s unintuitive UI made it difficult to navigate and work with.
### **Challenges**
We had to overcome these difficulties in various ways. Firstly, we had to play around with Jira for a while before figuring out how we were supposed to use it for bug reports. This allowed us to figure out the specifics, like how to create bug reports and sort them. We also re-read the instructions and discussed as a group how we should work, and what we should do. This also allowed us to develop various protocols such as how to classify bugs if they were encapsulated in each other.
### **Lessons Learned**
Overall, we learned some of the basics of using Jira, as well as some skills to learn and start using software that we were unfamiliar with. We were also initially ready to get right into the assignment, rather than allowing ourselves to understand the instructions and devise a plan of action. This is something that we ended up regretting, as the assignment took us a lot longer than it should have. 

# Comments/feedback on the lab and lab document itself
Overall, we found this lab quite educational in terms of simulating a real world testing environment. However, we found a few issues with the way it was designed.The instructions were somewhat unclear and unorganized, making it more difficult for us to understand how to carry out the assignment. Additionally, very few instructions were provided on how to utilize Jira, making it quite time-consuming for us to play around with it and figure it out on our own. The demo section itself also felt a little bit disorganized as we did not have a dedicated schedule, and did not really know what we were tested on. Having more information about the lab session prior to it would have have been great, as well as having a schedule for when groups would present so students are not simply waiting for TAs.
